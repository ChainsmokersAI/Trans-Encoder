{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a7afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8ee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Hyperparams\n",
    "max_sent_len=256\n",
    "batch_size=16\n",
    "accum_steps=1\n",
    "lr=5e-5\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7070fec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-Trained Tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Pre-Trained LM\n",
    "pretrained=AutoModel.from_pretrained(\"roberta-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb4dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBenchmark(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data=[]\n",
    "        self.label=[]\n",
    "        \n",
    "        # Read Dataset\n",
    "        with open(path, \"r\") as f:\n",
    "            data=f.read().split(\"\\n\")\n",
    "            f.close()\n",
    "        # Remove Empty Data\n",
    "        data.remove(\"\")\n",
    "        \n",
    "        for _data in data:\n",
    "            label, sent0, sent1=_data.split(\"\\t\")[4:7]\n",
    "            \n",
    "            # Encode Sentence\n",
    "            enc0=tokenizer.encode(sent0, truncation=True, max_length=max_sent_len)\n",
    "            enc1=tokenizer.encode(sent1, truncation=True, max_length=max_sent_len)\n",
    "            \n",
    "            # Append Data\n",
    "            self.data.append(enc0[:-1]+[tokenizer.sep_token_id]+enc1[1:])\n",
    "            self.label.append(float(label))\n",
    "            \n",
    "        print(len(self.data), \"data\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed05dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_seq_len=0\n",
    "    for data, _ in batch:\n",
    "        if len(data)>max_seq_len: max_seq_len=len(data)\n",
    "            \n",
    "    batch_data=[]\n",
    "    batch_label=[]\n",
    "    for data, label in batch:\n",
    "        data.extend([tokenizer.pad_token_id]*(max_seq_len-len(data)))\n",
    "        batch_data.append(data)\n",
    "        \n",
    "        batch_label.append(label)\n",
    "        \n",
    "    return torch.tensor(batch_data), torch.tensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba0fa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749 data\n"
     ]
    }
   ],
   "source": [
    "# STS Benchmark Train Set\n",
    "dataset_train=STSBenchmark(path=\"../dataset/stsbenchmark/sts-train.csv\")\n",
    "dataloader_train=DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc686c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP\n",
    "        self.pooler=nn.Linear(pretrained.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pretrained(x)\n",
    "        cls=x.last_hidden_state[:,0,:]\n",
    "        return self.pooler(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376c23de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "# Model: Cross-Encoder\n",
    "model=CrossEncoder(pretrained=pretrained).to(device)\n",
    "model.train()\n",
    "\n",
    "# Loss: MSE\n",
    "mse_loss=nn.MSELoss()\n",
    "\n",
    "# Optimizer, Scheduler\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=int(epochs*len(dataset_train)/(accum_steps*batch_size))\n",
    ")\n",
    "\n",
    "# Mixed Precision: GradScaler\n",
    "scaler=amp.GradScaler()\n",
    "\n",
    "# Tensorboard\n",
    "writer=SummaryWriter()\n",
    "\n",
    "step_global=0\n",
    "for epoch in range(epochs):\n",
    "    _loss=0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (data, label) in enumerate(dataloader_train):\n",
    "        # Load Data, Label\n",
    "        data=data.to(device)\n",
    "        label=label.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        with amp.autocast():\n",
    "            pred=model(data)\n",
    "            loss=mse_loss(pred, label.unsqueeze(-1))/accum_steps\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        _loss+=loss.item()\n",
    "        \n",
    "        # Step\n",
    "        if (step+1)%accum_steps==0:\n",
    "            step_global+=1\n",
    "            \n",
    "            # Tensorboard\n",
    "            writer.add_scalar(\n",
    "                f'loss_train/Cross-Encoder_batch{int(accum_steps*batch_size)}_lr{lr}_epochs{epochs}',\n",
    "                _loss,\n",
    "                step_global\n",
    "            )\n",
    "            _loss=0\n",
    "            \n",
    "            # Optimizer, Scheduler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    # Save Model\n",
    "    model.to(torch.device('cpu'))\n",
    "    torch.save(\n",
    "        model,\n",
    "        f'../model/Cross-Encoder_batch{int(accum_steps*batch_size)}_lr{lr}_epoch{epoch+1}of{epochs}'\n",
    "    )\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e546b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "698698a5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cece9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447c3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP\n",
    "        self.pooler=nn.Linear(pretrained.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pretrained(x)\n",
    "        cls=x.last_hidden_state[:,0,:]\n",
    "        return self.pooler(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec33ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:3\")\n",
    "\n",
    "# Pre-Trained Tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load Trained Model: Cross-Encoder\n",
    "model=torch.load(\"../model/Cross-Encoder_batch16_lr5e-05_epoch5of5\").to(device)\n",
    "model.eval()\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbb12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS Benchmark Test Set\n",
    "with open('../dataset/stsbenchmark/sts-test.csv', 'r') as f:\n",
    "    stsb_test=f.read()\n",
    "    f.close()\n",
    "    \n",
    "preds=[]\n",
    "labels=[]\n",
    "for data in stsb_test.split(\"\\n\")[:-1]:\n",
    "    label, sent0, sent1=data.split(\"\\t\")[4:7]\n",
    "    labels.append(float(label))\n",
    "    \n",
    "    # Encode Sentence\n",
    "    enc0=tokenizer.encode(sent0)\n",
    "    enc1=tokenizer.encode(sent1)\n",
    "    \n",
    "    # Forward\n",
    "    input_=torch.tensor([enc0[:-1]+[tokenizer.sep_token_id]+enc1[1:]])\n",
    "    pred=model(input_.to(device))\n",
    "    \n",
    "    preds.append(pred[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6eefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.89041308],\n",
       "       [0.89041308, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a111478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.8837970016428108, pvalue=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc96bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
