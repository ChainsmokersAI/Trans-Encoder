{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbd5006",
   "metadata": {},
   "source": [
    "### Create Pseudo-Labels using Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb623b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ea91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP\n",
    "        self.pooler=nn.Linear(pretrained.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pretrained(x)\n",
    "        cls=x.last_hidden_state[:,0,:]\n",
    "        return self.pooler(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32399800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Checkpoint: Distilled Cross-Encoder\n",
    "tokenizer_cross=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "cross_enc=torch.load(\"../model/Cross-Encoder-Distilled_batch16_lr5e-05_epoch4of5\").to(device)\n",
    "cross_enc.eval()\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28535fe",
   "metadata": {},
   "source": [
    "### How to Make Sentence Pairs\n",
    "### a. Same Pairs in Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03387020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(dataset_path, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Same Pairs in Train Set\n",
    "    \"\"\"\n",
    "    # Read Dataset\n",
    "    if \"bi2cross\" in dataset_path:\n",
    "        df=pd.read_csv(dataset_path)\n",
    "        \n",
    "    # Pseudo-Labeling\n",
    "    pseudo_labels=[]\n",
    "    for idx in df.index:\n",
    "        # Encode Sentence\n",
    "        enc0=tokenizer.encode(df.loc[idx][\"sent0\"])\n",
    "        enc1=tokenizer.encode(df.loc[idx][\"sent1\"])\n",
    "        \n",
    "        # Forward\n",
    "        _input=torch.tensor([enc0[:-1]+[tokenizer.sep_token_id]+enc1[1:]])\n",
    "        pred=model(_input.to(device))\n",
    "        \n",
    "        pseudo_labels.append(pred[0].item())\n",
    "        \n",
    "    # Update Column\n",
    "    df[\"pseudo_label\"]=pseudo_labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ab82a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "      <th>pseudo_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>3.024760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.677473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>2.815390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.189032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>2.909889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sent0  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "\n",
       "                                               sent1  pseudo_label  \n",
       "0                        An air plane is taking off.      3.024760  \n",
       "1                          A man is playing a flute.      3.677473  \n",
       "2  A man is spreading shredded cheese on an uncoo...      2.815390  \n",
       "3                         Two men are playing chess.      2.189032  \n",
       "4                 A man seated is playing the cello.      2.909889  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pseudo_label(\n",
    "    dataset_path=\"../dataset/bi2cross-sts-train.csv\",\n",
    "    tokenizer=tokenizer_cross,\n",
    "    model=cross_enc\n",
    ")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4e0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"pseudo_label\"], bins=10, range=(0,1))\n",
    "plt.title(\"Distribution of Pseudo-Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6799ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset\n",
    "df.to_csv(\"../dataset/cross2bi-sts-train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4517b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf9b3c3",
   "metadata": {},
   "source": [
    "### Train Bi-Encoder with Pseudo-Labels & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d466d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Hyperparams\n",
    "max_sent_len=512\n",
    "batch_size=16\n",
    "accum_steps=1\n",
    "lr=1e-7\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b523cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabelDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        # Data: Sentence Pair\n",
    "        self.sent0=[]\n",
    "        self.sent1=[]\n",
    "        # Label\n",
    "        self.label=[]\n",
    "        \n",
    "        # Read Dataset\n",
    "        df=pd.read_csv(path)\n",
    "        \n",
    "        for idx in df.index:\n",
    "            row=df.loc[idx]\n",
    "            \n",
    "            # Encode Sentence\n",
    "            enc0=tokenizer.encode(row[\"sent0\"], truncation=True, max_length=max_sent_len)\n",
    "            enc1=tokenizer.encode(row[\"sent1\"], truncation=True, max_length=max_sent_len)\n",
    "            \n",
    "            # Append Data\n",
    "            self.sent0.append(enc0)\n",
    "            self.sent1.append(enc1)\n",
    "            # Append Label\n",
    "            self.label.append(float(row[\"pseudo_label\"]))\n",
    "            \n",
    "        print(len(self.sent0), \"data\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent0[idx], self.sent1[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e6c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        # Max Sentence Length\n",
    "        max_seq_len0=0\n",
    "        max_seq_len1=0\n",
    "        for sent0, sent1, _ in batch:\n",
    "            if len(sent0)>max_seq_len0: max_seq_len0=len(sent0)\n",
    "            if len(sent1)>max_seq_len1: max_seq_len1=len(sent1)\n",
    "\n",
    "        # Data: Sentence Pair\n",
    "        batch_sent0=[]\n",
    "        batch_sent1=[]\n",
    "        # Label\n",
    "        batch_label=[]\n",
    "        for sent0, sent1, label in batch:\n",
    "            sent0.extend([tokenizer.pad_token_id]*(max_seq_len0-len(sent0)))\n",
    "            batch_sent0.append(sent0)\n",
    "            \n",
    "            sent1.extend([tokenizer.pad_token_id]*(max_seq_len1-len(sent1)))\n",
    "            batch_sent1.append(sent1)\n",
    "\n",
    "            batch_label.append(label)\n",
    "\n",
    "        return torch.tensor(batch_sent0), torch.tensor(batch_sent1), torch.tensor(batch_label)\n",
    "    \n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653fa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Cosine Similarity\n",
    "        self.cos_sim=nn.CosineSimilarity()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1=self.pretrained(x1)\n",
    "        hidden1=x1.last_hidden_state\n",
    "        \n",
    "        x2=self.pretrained(x2)\n",
    "        hidden2=x2.last_hidden_state\n",
    "        \n",
    "        cos_sims=self.cos_sim(hidden1[:,0,:], hidden2[:,0,:]).unsqueeze(-1)\n",
    "        return cos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75d9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bi_encoder_with_pseudo_labels(pseudo_label_path, loss_func):\n",
    "    \"\"\"\n",
    "    Cross-Encoder -> Bi-Encoder Distillation\n",
    "    \"\"\"\n",
    "    # Pre-Trained Tokenizer\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\")\n",
    "    # Pseudo-Labeled Dataset\n",
    "    dataset_train=PseudoLabelDataset(path=pseudo_label_path, tokenizer=tokenizer)\n",
    "    dataloader_train=DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=get_collate_fn(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Pre-Trained LM\n",
    "    pretrained=AutoModel.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\").to(device)\n",
    "    # Model: Bi-Encoder\n",
    "    model=BiEncoder(pretrained=pretrained).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Loss: MSE\n",
    "    if loss_func==\"MSE\":\n",
    "        train_loss=nn.MSELoss()\n",
    "    # Loss: BCE\n",
    "    elif loss_func==\"BCE\":\n",
    "        train_loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    # Optimizer, Scheduler\n",
    "    optimizer=AdamW(model.parameters(), lr=lr)\n",
    "    scheduler=get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        # 5% of Total Step\n",
    "        num_warmup_steps=int(0.05*epochs*len(dataset_train)/(accum_steps*batch_size)),\n",
    "        num_training_steps=int(epochs*len(dataset_train)/(accum_steps*batch_size))\n",
    "    )\n",
    "\n",
    "    # Mixed Precision: GradScaler\n",
    "    scaler=amp.GradScaler()\n",
    "\n",
    "    # Tensorboard\n",
    "    writer=SummaryWriter()\n",
    "\n",
    "    step_global=0\n",
    "    for epoch in range(epochs):\n",
    "        _loss=0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, (sent0, sent1, label) in enumerate(dataloader_train):\n",
    "            # Load Data, Label\n",
    "            sent0=sent0.to(device)\n",
    "            sent1=sent1.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast():\n",
    "                pred=model(sent0, sent1)\n",
    "                loss=train_loss(pred, label.unsqueeze(-1))/accum_steps\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            _loss+=loss.item()\n",
    "\n",
    "            # Step\n",
    "            if (step+1)%accum_steps==0:\n",
    "                step_global+=1\n",
    "\n",
    "                # Tensorboard\n",
    "                writer.add_scalar(\n",
    "                    f'loss_train/Bi-Encoder-Distilled_batch{int(accum_steps*batch_size)}_lr{lr}_epochs{epochs}',\n",
    "                    _loss,\n",
    "                    step_global\n",
    "                )\n",
    "                _loss=0\n",
    "\n",
    "                # Optimizer, Scheduler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Save Model\n",
    "        model.to(torch.device(\"cpu\"))\n",
    "        torch.save(\n",
    "            model,\n",
    "            f'../model/Bi-Encoder-Distilled_batch{int(accum_steps*batch_size)}_lr{lr}_epoch{epoch+1}of{epochs}'\n",
    "        )\n",
    "        model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07170558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distilled_bi_encoder(model_path):\n",
    "    # Pre-Trained Tokenizer\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\")\n",
    "    # Load Trained Model: Bi-Encoder\n",
    "    model=torch.load(model_path).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # STS Benchmark Test Set\n",
    "    with open(\"../dataset/stsbenchmark/sts-test.csv\", \"r\") as f:\n",
    "        stsb_test=f.read()\n",
    "        f.close()\n",
    "\n",
    "    preds=[]\n",
    "    labels=[]\n",
    "    for data in stsb_test.split(\"\\n\")[:-1]:\n",
    "        label, sent0, sent1=data.split(\"\\t\")[4:7]\n",
    "        labels.append(float(label))\n",
    "\n",
    "        # Encode Sentence\n",
    "        enc0=tokenizer.encode(sent0)\n",
    "        enc1=tokenizer.encode(sent1)\n",
    "\n",
    "        # Forward\n",
    "        pred=model(torch.tensor([enc0]).to(device), torch.tensor([enc1]).to(device))\n",
    "\n",
    "        preds.append(pred[0].item())\n",
    "        \n",
    "    print(np.corrcoef(preds, labels))\n",
    "    print(stats.spearmanr(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3013540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Distillation\n",
    "train_bi_encoder_with_pseudo_labels(pseudo_label_path=\"../dataset/cross2bi-sts-train.csv\", loss_func=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9bb112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.81442487]\n",
      " [0.81442487 1.        ]]\n",
      "SpearmanrResult(correlation=0.8078585560432042, pvalue=1.8018e-318)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluate_distilled_bi_encoder(model_path=\"../model/Bi-Encoder-Distilled_batch16_lr1e-07_epoch3of5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068135fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
