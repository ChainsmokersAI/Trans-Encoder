{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5aaf3bd",
   "metadata": {},
   "source": [
    "### Load Bi-Encoder: Unsupervised SimCSE & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fdfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62649b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Checkpoint: Unsupervised SimCSE\n",
    "tokenizer_bi=AutoTokenizer.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\")\n",
    "bi_enc=AutoModel.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\").to(device)\n",
    "bi_enc.eval()\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4e0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS Benchmark Test Set\n",
    "with open(\"../dataset/stsbenchmark/sts-test.csv\", \"r\") as f:\n",
    "    data=f.read().split(\"\\n\")\n",
    "    data.remove(\"\")\n",
    "    f.close()\n",
    "    \n",
    "preds=[]\n",
    "labels=[]\n",
    "for _data in data:\n",
    "    label, sent0, sent1=_data.split(\"\\t\")[4:7]\n",
    "    labels.append(float(label))\n",
    "    \n",
    "    # Encode Sentence\n",
    "    enc0=tokenizer_bi.encode(sent0)\n",
    "    enc1=tokenizer_bi.encode(sent1)\n",
    "    \n",
    "    # Forward\n",
    "    cls0=bi_enc(torch.tensor([enc0]).to(device)).last_hidden_state[:,0,:]\n",
    "    cls1=bi_enc(torch.tensor([enc1]).to(device)).last_hidden_state[:,0,:]\n",
    "    \n",
    "    pred=1-spatial.distance.cosine(np.array(cls0.detach().cpu()), np.array(cls1.detach().cpu()))\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d181de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.81180198],\n",
       "       [0.81180198, 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21d86ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.8009763763165447, pvalue=4.38233081262766e-309)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab21dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e192b5",
   "metadata": {},
   "source": [
    "### Create Pseudo-Labels using Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a1f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48202899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Checkpoint: Unsupervised SimCSE\n",
    "tokenizer_bi=AutoTokenizer.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\")\n",
    "bi_enc=AutoModel.from_pretrained(\"princeton-nlp/unsup-simcse-roberta-base\").to(device)\n",
    "bi_enc.eval()\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb5c79",
   "metadata": {},
   "source": [
    "### How to Make Sentence Pairs:\n",
    "### a. Same Pairs in STS-B Train Set (NOT Use Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52629ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(dataset, tokenizer, model):\n",
    "    # Read Dataset\n",
    "    if \"stsbenchmark\" in dataset:\n",
    "        data=open(dataset).read().split(\"\\n\")\n",
    "        data.remove(\"\")\n",
    "        \n",
    "        sents0=[]\n",
    "        sents1=[]\n",
    "        \n",
    "        # Parse Dataset\n",
    "        for _data in data:\n",
    "            sent0, sent1=_data.split(\"\\t\")[5:7]\n",
    "            \n",
    "            sents0.append(sent0)\n",
    "            sents1.append(sent1)\n",
    "            \n",
    "        # Make DataFrame\n",
    "        df=pd.DataFrame({\n",
    "            \"sent0\": sents0,\n",
    "            \"sent1\": sents1,\n",
    "        })\n",
    "        \n",
    "    pseudo_labels=[]\n",
    "    \n",
    "    # Pseudo-Labeling\n",
    "    for idx in df.index:\n",
    "        row=df.loc[idx]\n",
    "        \n",
    "        # Encode Sentence\n",
    "        enc0=tokenizer.encode(row[\"sent0\"])\n",
    "        enc1=tokenizer.encode(row[\"sent1\"])\n",
    "        \n",
    "        # Forward\n",
    "        cls0=model(torch.tensor([enc0]).to(model.device)).last_hidden_state[:,0,:]\n",
    "        cls1=model(torch.tensor([enc1]).to(model.device)).last_hidden_state[:,0,:]\n",
    "        \n",
    "        pred=1-spatial.distance.cosine(np.array(cls0.detach().cpu()), np.array(cls1.detach().cpu()))\n",
    "        pseudo_labels.append(pred)\n",
    "        \n",
    "    # Append Pseudo-Label Column\n",
    "    df[\"pseudo_label\"]=pseudo_labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c81036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "      <th>pseudo_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>0.982021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>0.969140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>0.920545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>0.891384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>0.910757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sent0  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "\n",
       "                                               sent1  pseudo_label  \n",
       "0                        An air plane is taking off.      0.982021  \n",
       "1                          A man is playing a flute.      0.969140  \n",
       "2  A man is spreading shredded cheese on an uncoo...      0.920545  \n",
       "3                         Two men are playing chess.      0.891384  \n",
       "4                 A man seated is playing the cello.      0.910757  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pseudo_label(\n",
    "    dataset=\"../dataset/stsbenchmark/sts-train.csv\",\n",
    "    tokenizer=tokenizer_bi,\n",
    "    model=bi_enc\n",
    ")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cc120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset\n",
    "df.to_csv(\"../dataset/bi2cross-sts-train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159083f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce27a495",
   "metadata": {},
   "source": [
    "### Train Cross-Encoder with Pseudo-Labels & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3503803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1efc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device: GPU\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "# Hyperparams\n",
    "max_sent_len=256\n",
    "batch_size=16\n",
    "accum_steps=1\n",
    "lr=5e-5\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26dfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabelDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        self.data=[]\n",
    "        self.label=[]\n",
    "        \n",
    "        # Read Dataset\n",
    "        df=pd.read_csv(path)\n",
    "        \n",
    "        for idx in df.index:\n",
    "            row=df.loc[idx]\n",
    "            \n",
    "            # Encode Sentence\n",
    "            enc0=tokenizer.encode(row[\"sent0\"], truncation=True, max_length=max_sent_len)\n",
    "            enc1=tokenizer.encode(row[\"sent1\"], truncation=True, max_length=max_sent_len)\n",
    "            \n",
    "            # Append Data\n",
    "            self.data.append(enc0[:-1]+[tokenizer.sep_token_id]+enc1[1:])\n",
    "            self.label.append(float(row[\"pseudo_label\"]))\n",
    "            \n",
    "        print(len(self.data), \"data\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f20f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        max_seq_len=0\n",
    "        for data, _ in batch:\n",
    "            if len(data)>max_seq_len: max_seq_len=len(data)\n",
    "\n",
    "        batch_data=[]\n",
    "        batch_label=[]\n",
    "        for data, label in batch:\n",
    "            data.extend([tokenizer.pad_token_id]*(max_seq_len-len(data)))\n",
    "            batch_data.append(data)\n",
    "\n",
    "            batch_label.append(label)\n",
    "\n",
    "        return torch.tensor(batch_data), torch.tensor(batch_label)\n",
    "    \n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa42bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP\n",
    "        self.pooler=nn.Linear(pretrained.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pretrained(x)\n",
    "        cls=x.last_hidden_state[:,0,:]\n",
    "        return self.pooler(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e048bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_encoder_with_pseudo_labels(pseudo_label_path, loss_func=\"MSE\"):\n",
    "    \"\"\"\n",
    "    Bi-Encoder -> Cross-Encoder Distillation\n",
    "    \"\"\"\n",
    "    # Pre-Trained Tokenizer\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    # Pseudo-Labeled Dataset\n",
    "    dataset_train=PseudoLabelDataset(path=pseudo_label_path, tokenizer=tokenizer)\n",
    "    dataloader_train=DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=get_collate_fn(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Pre-Trained LM\n",
    "    pretrained=AutoModel.from_pretrained(\"roberta-base\").to(device)\n",
    "    # Model: Cross-Encoder\n",
    "    model=CrossEncoder(pretrained=pretrained).to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Loss: MSE\n",
    "    train_loss=nn.MSELoss()\n",
    "\n",
    "    # Optimizer, Scheduler\n",
    "    optimizer=AdamW(model.parameters(), lr=lr)\n",
    "    scheduler=get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=int(epochs*len(dataset_train)/(accum_steps*batch_size))\n",
    "    )\n",
    "\n",
    "    # Mixed Precision: GradScaler\n",
    "    scaler=amp.GradScaler()\n",
    "\n",
    "    # Tensorboard\n",
    "    writer=SummaryWriter()\n",
    "\n",
    "    step_global=0\n",
    "    for epoch in range(epochs):\n",
    "        _loss=0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, (data, label) in enumerate(dataloader_train):\n",
    "            # Load Data, Label\n",
    "            data=data.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast():\n",
    "                pred=model(data)\n",
    "                loss=train_loss(pred, label.unsqueeze(-1))/accum_steps\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            _loss+=loss.item()\n",
    "\n",
    "            # Step\n",
    "            if (step+1)%accum_steps==0:\n",
    "                step_global+=1\n",
    "\n",
    "                # Tensorboard\n",
    "                writer.add_scalar(\n",
    "                    f'loss_train/Cross-Encoder-Distilled(STS)_batch{int(accum_steps*batch_size)}_lr{lr}_epochs{epochs}',\n",
    "                    _loss,\n",
    "                    step_global\n",
    "                )\n",
    "                _loss=0\n",
    "\n",
    "                # Optimizer, Scheduler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Save Model\n",
    "        model.to(torch.device('cpu'))\n",
    "        torch.save(\n",
    "            model,\n",
    "            f'../model/Cross-Encoder-Distilled(STS)_batch{int(accum_steps*batch_size)}_lr{lr}_epoch{epoch+1}of{epochs}'\n",
    "        )\n",
    "        model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b671b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distilled_cross_encoder(model_path):\n",
    "    # Pre-Trained Tokenizer\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    # Load Trained Model: Cross-Encoder\n",
    "    model=torch.load(model_path).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # STS Benchmark Test Set\n",
    "    with open('../dataset/stsbenchmark/sts-test.csv', 'r') as f:\n",
    "        stsb_test=f.read()\n",
    "        f.close()\n",
    "\n",
    "    preds=[]\n",
    "    labels=[]\n",
    "    for data in stsb_test.split(\"\\n\")[:-1]:\n",
    "        label, sent0, sent1=data.split(\"\\t\")[4:7]\n",
    "        labels.append(float(label))\n",
    "\n",
    "        # Encode Sentence\n",
    "        enc0=tokenizer.encode(sent0)\n",
    "        enc1=tokenizer.encode(sent1)\n",
    "\n",
    "        # Forward\n",
    "        input_=torch.tensor([enc0[:-1]+[tokenizer.sep_token_id]+enc1[1:]])\n",
    "        pred=model(input_.to(device))\n",
    "\n",
    "        preds.append(pred[0].item())\n",
    "        \n",
    "    print(np.corrcoef(preds, labels))\n",
    "    print(stats.spearmanr(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19229c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.83768211]\n",
      " [0.83768211 1.        ]]\n",
      "SpearmanrResult(correlation=0.825565631014563, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Case a. Same Pairs in STS-B Train Set\n",
    "train_cross_encoder_with_pseudo_labels(pseudo_label_path=\"../dataset/bi2cross-sts-train.csv\")\n",
    "evaluate_distilled_cross_encoder(model_path=\"../model/Cross-Encoder-Distilled(STS)_batch16_lr5e-05_epoch5of5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e75f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
